{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать ваасермановский лосс\n",
    "Разобраться с dimm - что ето?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxbostick\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b388842e7f4e7a800c23aed37959bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333333327028, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\work\\GAN_Z\\wandb\\run-20230831_114257-smdpevfd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xbostick/GAN-Z/runs/smdpevfd' target=\"_blank\">ethereal-monkey-24</a></strong> to <a href='https://wandb.ai/xbostick/GAN-Z' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xbostick/GAN-Z' target=\"_blank\">https://wandb.ai/xbostick/GAN-Z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xbostick/GAN-Z/runs/smdpevfd' target=\"_blank\">https://wandb.ai/xbostick/GAN-Z/runs/smdpevfd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xbostick/GAN-Z/runs/smdpevfd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f89eebe910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GAN-Z\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"GAN\",\n",
    "    \"dataset\": \"chr1\",\n",
    "    \"epochs\": 300,\n",
    "    }\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False # GPU Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = False # Generate generic data on the fly (ignores data_loc and data_start args)\n",
    "data_loc = Path(\"./data/\") # Data location\n",
    "epoch = 300\n",
    "disc_iters = 5 # Number of iterations to train discriminator for at each epoch\n",
    "latent_dim = 5 # Size of latent space\n",
    "gen_dim = 100 # Generator dimension parameter\n",
    "disc_dim = 100 # Discriminator dimension parameter\n",
    "gen_layers = 5\n",
    "disc_layer = 5\n",
    "batch_size = 100\n",
    "max_seq_len = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gen_data = lib.models.resnet_generator(latent_vars, args.gen_dim, args.max_seq_len, data_enc_dim, args.annotate)\n",
    "  data_enc_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_vocab = {\"A\":0,\n",
    "             \"C\":1,\n",
    "             \"G\":2,\n",
    "             \"T\":3,\n",
    "             \"N\":4} # catch-all auxiliary token\n",
    "\n",
    "class GenomicData(Dataset):\n",
    "    def __init__(self, path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        with open(path) as fasta_file:  # Will close handle cleanly\n",
    "            seq_record = {}\n",
    "            seq_record[\"lengths\"] = []\n",
    "            seq_record[\"seq\"] = []\n",
    "\n",
    "            for seq_rec in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "                seq_record[\"lengths\"].append(len(seq_rec.seq))\n",
    "                seq_record[\"seq\"].append(seq_rec.seq)\n",
    "        self.len = int(seq_record[\"lengths\"][0])\n",
    "        self.coded_seq = []\n",
    "        for one in list(seq_record[\"seq\"][0]):\n",
    "            try:\n",
    "                self.coded_seq.append(dna_vocab[one.upper()])\n",
    "            except:\n",
    "                print(one)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.len/self.seq_len)#int(np.round((self.len - self.seq_len)/100))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.coded_seq[idx* self.seq_len:(idx+1)*self.seq_len])\n",
    "\n",
    "\n",
    "GenData = GenomicData(\"./data/chr1.fa\", max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 сверточных слоев с skip connection residual factor 0.3.\n",
    "В сверточном слое 100 фильтров ядро 5 stribe 1\n",
    "adam optimazer и wasserstein loss(WGAN) and lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_new(nn.Module):\n",
    "    def __init__(self,  latent_vars, gen_dim, max_seq_len, annotated=False, res_layers=5):\n",
    "        super(Generator_new, self).__init__()\n",
    "        self.input_size = latent_vars.shape\n",
    "        self.output_size = gen_dim * max_seq_len\n",
    "        self.seq_len = max_seq_len\n",
    "        self.gen_dim = gen_dim\n",
    "        self.res_layer = res_layers\n",
    "        #self.conv1 = nn.Conv1d(3,6,5,1)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv1d(1,gen_dim,5,1, padding = 'same'),\n",
    "                        nn.ReLU())\n",
    "        self.con1 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con2 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con3 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con4 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con5 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv1d(gen_dim,1,5,1, padding = 'same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Softmax())\n",
    "        # self.con_array = [\n",
    "        #     self.con1,\n",
    "        #     self.con2,\n",
    "        #     self.con3,\n",
    "        #     self.con4,\n",
    "        #     self.con5,\n",
    "        # ]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Softmax()#nn.Sigmoid()\n",
    "        self.linear = nn.Linear(100,1)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        _input = self.conv1(x)#torch.reshape(x,(-1,self.gen_dim,self.seq_len))     \n",
    "        \n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con2(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con3(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con4(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con5(_input)\n",
    "        out = self.relu(out)\n",
    "        out = self.sigmoid(_input + 0.3 * out)  \n",
    "        # for i in range(self.res_layer):\n",
    "        #     output = self.res_block(_input.float(),i, 0.3)\n",
    "        #     _input = output\n",
    "        #output = self.conv2(out)\n",
    "        output = torch.transpose(out, 0, 1)\n",
    "        output = self.linear(output) \n",
    "        return torch.transpose(output, 0, 1)\n",
    "    \n",
    "    def res_block(self,_input, index, downsample = 0):\n",
    "        res = _input\n",
    "        out = self.con_array[index](res)\n",
    "        out = self.relu(out)\n",
    "        if downsample:\n",
    "            res = res * downsample\n",
    "            out = out + res\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critical_new(nn.Module):\n",
    "    def __init__(self,  latent_vars, gen_dim, max_seq_len, annotated=False, res_layers=5):\n",
    "        super(Critical_new, self).__init__()\n",
    "        self.input_size = latent_vars.shape\n",
    "        self.output_size = gen_dim * max_seq_len\n",
    "        self.seq_len = max_seq_len\n",
    "        self.gen_dim = gen_dim\n",
    "        self.res_layer = res_layers\n",
    "        #self.conv1 = nn.Conv1d(3,6,5,1)\n",
    "        self.conv1 = nn.Conv1d(gen_dim,1,5,1, padding = 'same')\n",
    "        self.con1 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con2 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con3 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con4 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con5 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv1d(1, gen_dim,5,1, padding = 'same'),\n",
    "                        nn.ReLU())\n",
    "        # self.con_array = [\n",
    "        #     self.con1,\n",
    "        #     self.con2,\n",
    "        #     self.con3,\n",
    "        #     self.con4,\n",
    "        #     self.con5,\n",
    "        # ]\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        _input = x\n",
    "        out = self.conv2(_input)\n",
    "        \n",
    "        out = self.con1(out)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con2(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con3(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con4(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con5(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        # for i in range(self.res_layer):\n",
    "        #     output = self.res_block(_input.float(),i, 0.3)\n",
    "        #     _input = output\n",
    "        # #output = torch.reshape(output,(-1,1))\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        return out #self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = torch.randn([10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator= Generator_new(latent_vars,gen_dim,max_seq_len)\n",
    "discriminator = Critical_new(latent_vars,gen_dim,max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.cuda()\n",
    "# out = generator(z)\n",
    "# discriminator.cuda()\n",
    "# discriminator(out).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    #adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested default - beta parameters (decay of first order momentum of gradients)\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "# suggested default - learning rate\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas = (b1,b2) )\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latent_dim = 1\n",
    "wandb.watch(generator, log_freq=100)\n",
    "wandb.watch(discriminator, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "        # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_data))\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = Variable(Tensor(1,1000).fill_(1.0), requires_grad=False)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a0cca2f8494e038b7f6b89034ac04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_24080\\1990929764.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.sigmoid(_input + 0.3 * out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.12158671021461487; G_loss: 0.017218194901943207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b433d9dd294f1d8cb0bee460d2435a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.05463799834251404; G_loss: 0.013123518787324429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f79e7bb2284c4daab0f00c80bc7f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.0707850530743599; G_loss: 0.015960201621055603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e43a4e8a744274bea7b71459119995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.05298873037099838; G_loss: 0.015086311846971512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78184cacdb604b258707eba0ece0cb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: -0.05014872923493385; G_loss: 0.008796975016593933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a874b9c28372480aa04278c9f075808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.0378289595246315; G_loss: 0.009645611979067326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80b5f2db79f478bbb603b11300f17fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 1.2333124876022339; G_loss: 0.012741897255182266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1f2a3ec1546a8809a055355e2882a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.061565786600112915; G_loss: 0.01131651271134615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a457568f94b84c3d9a3fe16ed1bbb4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.9903973340988159; G_loss: 0.012167957611382008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f8726c04154ecda62a20931a23688d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.09718280285596848; G_loss: -9.798670362215489e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1c9a8208234534859462728a06396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.057243332266807556; G_loss: 0.007839877158403397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b224b8e526db4434bfcc52b9d41bdb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.035766225308179855; G_loss: -0.0009686851990409195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2254c5ed232f4afd9b12faf98a24f332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.10831856727600098; G_loss: 0.001587801263667643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baddebe1c4134d0f94d828c818363330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.16662704944610596; G_loss: -0.007629492320120335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dc2f778c5b4a8b8794366da4cf45a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: -0.02687123790383339; G_loss: -0.0033312449231743813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead8d23f7c4f4b3fa4fa687f5acb0650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 200 # suggested default = 200\n",
    "for epoch in range(n_epochs):\n",
    "    dataloader = None\n",
    "    dataloader = DataLoader(GenData,6,drop_last=True, shuffle = True)\n",
    "    for i in tqdm(range(int(np.floor(len(GenData)/300)))):\n",
    "        train_data = next(iter(dataloader)).type(Tensor)\n",
    "        train_data.cuda()\n",
    "        #   Critical forward-loss-backward-update\n",
    "        for j in range(5):\n",
    "\n",
    "            # Sample data\n",
    "            noise = Variable(torch.randn((1,1000))).cuda()# Random sampling Tensor(batch_size, latent_dim) of Gaussian distribution\n",
    "        \n",
    "            real_data = torch.reshape(train_data[j],(1,1000))\n",
    "            real_data = Variable(real_data)\n",
    "\n",
    "            #generic\n",
    "            fake_data = generator(noise)\n",
    "            critics_real = discriminator(real_data)\n",
    "            critics_fake = discriminator(fake_data)\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "            critics_loss = -torch.mean(critics_real) + torch.mean(critics_fake) + 10 * gradient_penalty\n",
    "            \n",
    "            critics_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            # Housekeeping - reset gradient\n",
    "            reset_grad(generator)\n",
    "            reset_grad(discriminator)\n",
    "\n",
    "        #   Generator forward-loss-backward-update\n",
    "        \n",
    "        noise = Variable(torch.randn((1,1000))).cuda()# Random sampling Tensor(batch_size, latent_dim) of Gaussian distribution\n",
    "        real_data = torch.reshape(train_data[j+1],(1,1000))\n",
    "        real_data = Variable(real_data)\n",
    "        #real_data = imgs[\"generator\"].type(Tensor)\n",
    "\n",
    "        fake_data = generator(noise)\n",
    "        critics_fake = discriminator(fake_data)\n",
    "\n",
    "        generator_loss = -torch.mean(critics_fake)\n",
    "        \n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        reset_grad(generator)\n",
    "        reset_grad(discriminator)\n",
    "    \n",
    "    table = wandb.Histogram(np_histogram = np.histogram(generator(noise).detach().cpu().numpy()))\n",
    "    wandb.log({\"Epoch\": epoch+1, \"Critics_loss\": critics_loss.item(), \"Generator_loss\": generator_loss.item(),\"Results\": table})\n",
    "\n",
    "    print('D_loss: {}; G_loss: {}'\n",
    "              .format(critics_loss.item(),  generator_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Critics_loss</td><td>▁▁▄▆▁▂██▄</td></tr><tr><td>Epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>Generator_loss</td><td>███▇▅▅▅▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Critics_loss</td><td>0.18323</td></tr><tr><td>Epoch</td><td>9</td></tr><tr><td>Generator_loss</td><td>-0.0262</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-tree-23</strong> at: <a href='https://wandb.ai/xbostick/GAN-Z/runs/hjdbxjf8' target=\"_blank\">https://wandb.ai/xbostick/GAN-Z/runs/hjdbxjf8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230830_212915-hjdbxjf8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_19292\\1990929764.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.sigmoid(_input + 0.3 * out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.3573,  -0.8689,   0.2615,  -0.2626,  -0.2750,  -0.2750,  -0.2745,\n",
       "          -0.2715,  -0.2750,  -0.2750,  -0.2743,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2747,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2633,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2725,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2737,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,\n",
       "          -0.2750,  -0.2739,  -0.2749,  -0.2750,  -0.2750,  -0.2741,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2743,  -0.2750,  -0.2750,  -0.2745,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2718,  -0.2750,  -0.2750,  -0.2705,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2746,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2742,  -0.2750,  -0.2750,  -0.2750,  -0.2690,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2735,\n",
       "          -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2750,  -0.2708,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2732,  -0.2750,  -0.2750,  -0.2736,\n",
       "          -0.2746,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2746,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2748,  -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2748,  -0.2750,  -0.2750,  -0.2746,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2740,  -0.2750,  -0.2750,  -0.2748,  -0.2738,  -0.2750,\n",
       "          -0.2750,  -0.2742,  -0.2748,  -0.2750,  -0.2750,  -0.2748,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2747,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2746,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2748,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2732,  -0.2750,  -0.2750,  -0.2726,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2659,  -0.2750,  -0.2750,  -0.2644,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2700,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2741,  -0.2750,  -0.2750,  -0.2747,  -0.2741,  -0.2750,  -0.2750,\n",
       "          -0.2714,  -0.2750,  -0.2750,  -0.2749,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2736,  -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2747,  -0.2750,  -0.2750,  -0.2746,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2733,  -0.2750,  -0.2750,  -0.2749,  -0.2747,\n",
       "          -0.2750,  -0.2750,  -0.2748,  -0.2748,  -0.2750,  -0.2750,  -0.2737,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2748,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2740,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2747,  -0.2750,  -0.2750,  -0.2745,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2734,  -0.2750,  -0.2750,  -0.2748,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2737,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2747,  -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2740,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2743,  -0.2750,  -0.2750,  -0.2748,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2721,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2740,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2747,  -0.2745,  -0.2750,  -0.2750,\n",
       "          -0.2709,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2748,  -0.2749,  -0.2750,  -0.2750,  -0.2743,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2744,  -0.2748,  -0.2750,  -0.2750,  -0.2742,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2731,  -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,\n",
       "          -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2740,  -0.2750,  -0.2750,  -0.2750,  -0.2743,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2742,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2688,  -0.2750,  -0.2750,  -0.2750,  -0.2660,\n",
       "          -0.2750,  -0.2750,  -0.2702,  -0.2748,  -0.2750,  -0.2750,  -0.2748,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2750,  -0.2735,\n",
       "          -0.2750,  -0.2750,  -0.2740,  -0.2750,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2730,  -0.2750,  -0.2750,  -0.2748,  -0.2745,\n",
       "          -0.2750,  -0.2750,  -0.2743,  -0.2748,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2745,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2748,  -0.2747,\n",
       "          -0.2750,  -0.2750,  -0.2739,  -0.2750,  -0.2750,  -0.2731,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2747,  -0.2750,\n",
       "          -0.2750,  -0.2725,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2742,  -0.2748,  -0.2750,\n",
       "          -0.2750,  -0.2736,  -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2741,  -0.2750,  -0.2750,  -0.2749,  -0.2737,  -0.2750,  -0.2750,\n",
       "          -0.2747,  -0.2747,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2744,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2746,  -0.2750,  -0.2750,  -0.2719,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2746,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2740,  -0.2750,  -0.2750,  -0.2750,  -0.2740,\n",
       "          -0.2750,  -0.2750,  -0.2744,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2734,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2743,  -0.2750,  -0.2750,  -0.2719,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2718,  -0.2750,  -0.2750,  -0.2711,  -0.2748,  -0.2750,\n",
       "          -0.2750,  -0.2672,  -0.2750,  -0.2750,  -0.2747,  -0.2737,  -0.2750,\n",
       "          -0.2750,  -0.2748,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2745,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2749,  -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2745,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2742,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2712,  -0.2750,  -0.2750,  -0.2740,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2747,  -0.2750,  -0.2750,  -0.2747,  -0.2750,  -0.2750,  -0.2744,\n",
       "          -0.2750,  -0.2750,  -0.2749,  -0.2748,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2748,  -0.2750,\n",
       "          -0.2750,  -0.2717,  -0.2750,  -0.2750,  -0.2749,  -0.2747,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2741,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2750,  -0.2745,  -0.2750,\n",
       "          -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2744,  -0.2750,  -0.2750,\n",
       "          -0.2711,  -0.2750,  -0.2750,  -0.2750,  -0.2716,  -0.2750,  -0.2750,\n",
       "          -0.2729,  -0.2749,  -0.2750,  -0.2750,  -0.2748,  -0.2750,  -0.2750,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2748,  -0.2750,  -0.2750,  -0.2744,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2747,  -0.2750,  -0.2750,  -0.2749,  -0.2750,  -0.2750,  -0.2749,\n",
       "          -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,  -0.2750,\n",
       "          -0.2738,  -0.2750,  -0.2750,  -0.2749,  -0.2749,  -0.2750,  -0.2750,\n",
       "          -0.2732,   1.2517,  -0.2751, -24.5527,  28.1135, -20.6638]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = Variable(torch.randn((1,1000))).cuda()\n",
    "generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 11439488.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 28732375.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12366876.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "test_data = datasets.MNIST(root = 'data', train = False,\n",
    "                           download = True, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = DataLoader(test_data,batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "         4, 0, 7, 4, 0, 1, 3, 1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: -0.006369204260408878; G_loss: 0.004545770585536957\n",
      "Iter-1000; D_loss: -0.004931438714265823; G_loss: -0.0005370582803152502\n",
      "Iter-2000; D_loss: -0.003440557047724724; G_loss: -0.0007516610203310847\n",
      "Iter-3000; D_loss: -0.0023505506105720997; G_loss: -0.0030477861873805523\n",
      "Iter-4000; D_loss: -0.0015784932766109705; G_loss: -0.003214463358744979\n",
      "Iter-5000; D_loss: -0.001160357380285859; G_loss: -0.0034737656824290752\n",
      "Iter-6000; D_loss: -0.001041144598275423; G_loss: -0.002607758389785886\n",
      "Iter-7000; D_loss: -0.0008465570863336325; G_loss: -0.0024224836379289627\n",
      "Iter-8000; D_loss: -0.0010717147961258888; G_loss: -0.0033117288257926702\n",
      "Iter-9000; D_loss: -0.0007010279223322868; G_loss: -0.0023659595753997564\n",
      "Iter-10000; D_loss: -0.0005162782035768032; G_loss: -0.001913088490255177\n",
      "Iter-11000; D_loss: -0.0006766270380467176; G_loss: -0.0031192232854664326\n",
      "Iter-12000; D_loss: -0.0007853535935282707; G_loss: -0.002375855576246977\n",
      "Iter-13000; D_loss: -0.0009073896799236536; G_loss: -0.002605010522529483\n",
      "Iter-14000; D_loss: -7.971515879034996e-05; G_loss: -0.0028412463143467903\n",
      "Iter-15000; D_loss: -0.0006263682153075933; G_loss: -0.003631618106737733\n",
      "Iter-16000; D_loss: -0.00039030960761010647; G_loss: -0.0030111721716821194\n",
      "Iter-17000; D_loss: -0.0005240221507847309; G_loss: -0.00286139571107924\n",
      "Iter-18000; D_loss: -0.0005264987703412771; G_loss: -0.002862904453650117\n",
      "Iter-19000; D_loss: -0.0006838657427579165; G_loss: -0.003025297774001956\n",
      "Iter-20000; D_loss: -0.00021736416965723038; G_loss: -0.0036035117227584124\n",
      "Iter-21000; D_loss: -0.00016752677038311958; G_loss: -0.0035822847858071327\n",
      "Iter-22000; D_loss: -0.00036074593663215637; G_loss: -0.0029321082402020693\n",
      "Iter-23000; D_loss: -0.0001396422740072012; G_loss: -0.003315031761303544\n",
      "Iter-24000; D_loss: -0.0001831422559916973; G_loss: -0.0029317254666239023\n",
      "Iter-25000; D_loss: -0.0004597916267812252; G_loss: -0.0032560001127421856\n",
      "Iter-26000; D_loss: 0.00014263205230236053; G_loss: -0.0030806821305304766\n",
      "Iter-27000; D_loss: -0.000226486474275589; G_loss: -0.0035687319468706846\n",
      "Iter-28000; D_loss: -0.00015907525084912777; G_loss: -0.003548889886587858\n",
      "Iter-29000; D_loss: -0.00022750860080122948; G_loss: -0.00464848754927516\n",
      "Iter-30000; D_loss: -0.0003921734169125557; G_loss: -0.0033187782391905785\n",
      "Iter-31000; D_loss: -0.0002801865339279175; G_loss: -0.003815828589722514\n",
      "Iter-32000; D_loss: -0.0003636549226939678; G_loss: -0.004024541936814785\n",
      "Iter-33000; D_loss: -0.00013002101331949234; G_loss: -0.00433571869507432\n",
      "Iter-34000; D_loss: -0.000438762828707695; G_loss: -0.003281509270891547\n",
      "Iter-35000; D_loss: -0.00036360369995236397; G_loss: -0.0036618136800825596\n",
      "Iter-36000; D_loss: -0.00044509186409413815; G_loss: -0.0036563887260854244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m     49\u001b[0m     \u001b[39m# Sample data\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     z \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mrandn(mb_size, z_dim))\n\u001b[1;32m---> 51\u001b[0m     X,_ \u001b[39m=\u001b[39m (\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(load)))\n\u001b[0;32m     52\u001b[0m     X \u001b[39m=\u001b[39m Variable(X)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     54\u001b[0m     \u001b[39m# Dicriminator forward-loss-backward-update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\mambaforge\\envs\\GAN\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:629\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 629\u001b[0m     \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_name):\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[0;32m    631\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[0;32m    632\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\mambaforge\\envs\\GAN\\Lib\\site-packages\\torch\\autograd\\profiler.py:495\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_enter_new(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m--> 495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit:\n\u001b[0;32m    497\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "mnist = test_data\n",
    "mb_size = 28\n",
    "z_dim = 10\n",
    "X_dim = 28\n",
    "y_dim = 1\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-4\n",
    "\n",
    "\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(z_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, X_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "G.cuda()\n",
    "\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, 1),\n",
    ")\n",
    "D.cuda()\n",
    "\n",
    "def reset_grad():\n",
    "    G.zero_grad()\n",
    "    D.zero_grad()\n",
    "\n",
    "\n",
    "G_solver = optim.RMSprop(G.parameters(), lr=lr)\n",
    "D_solver = optim.RMSprop(D.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for it in range(1000000):\n",
    "    for _ in range(5):\n",
    "        # Sample data\n",
    "        z = Variable(torch.randn(mb_size, z_dim))\n",
    "        X,_ = (next(iter(load)))\n",
    "        X = Variable(X).cuda()\n",
    "\n",
    "        # Dicriminator forward-loss-backward-update\n",
    "        G_sample = G(z.cuda())\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # Weight clipping\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    X, _ = next(iter(load))\n",
    "    X = Variable(X).cuda()\n",
    "    z = Variable(torch.randn(mb_size, z_dim)).cuda()\n",
    "\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = -torch.mean(D_fake)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    #Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'\n",
    "              .format(it, D_loss.cpu().data.numpy(), G_loss.cpu().data.numpy()))\n",
    "\n",
    "        samples = G(z).cpu().data.numpy()#[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        \n",
    "        plt.imshow(samples.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = torch.randn(1000, 100)\n",
    "layer(noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m noise\u001b[39m.\u001b[39;49mtranspose()\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "noise.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1000])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(noise, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, alpha, original):\n",
    "        super().__init__()\n",
    "        self.alpha =  alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_new(nn.Module):\n",
    "    def __init__(self,  latent_vars, gen_dim, max_seq_len, annotated=False, res_layers=5):\n",
    "        super(Generator_new, self).__init__()\n",
    "        self.input_size = latent_vars.shape\n",
    "        self.output_size = gen_dim * max_seq_len\n",
    "        self.seq_len = max_seq_len\n",
    "        self.gen_dim = gen_dim\n",
    "        self.res_layer = res_layers\n",
    "        #self.conv1 = nn.Conv1d(3,6,5,1)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv1d(1,gen_dim,5,1, padding = 'same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Sigmoid())\n",
    "        self.con1 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con2 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con3 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con4 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con5 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv1d(gen_dim,1,5,1, padding = 'same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Sigmoid())\n",
    "        # self.con_array = [\n",
    "        #     self.con1,\n",
    "        #     self.con2,\n",
    "        #     self.con3,\n",
    "        #     self.con4,\n",
    "        #     self.con5,\n",
    "        # ]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        G = torch.nn.Sequential(\n",
    "            nn.Conv1d(1,gen_dim,5,1, padding = 'same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same'),\n",
    "            \n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        _input = self.conv1(x)#torch.reshape(x,(-1,self.gen_dim,self.seq_len))     \n",
    "        \n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con2(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con3(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con4(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con5(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out   \n",
    "        # for i in range(self.res_layer):\n",
    "        #     output = self.res_block(_input.float(),i, 0.3)\n",
    "        #     _input = output\n",
    "        output = self.conv2(_input)\n",
    "        #output = self.relu(output)\n",
    "        return self.sigmoid(output)\n",
    "    \n",
    "    def res_block(self,_input, index, downsample = 0):\n",
    "        res = _input\n",
    "        out = self.con_array[index](res)\n",
    "        out = self.relu(out)\n",
    "        if downsample:\n",
    "            res = res * downsample\n",
    "            out = out + res\n",
    "        return out\n",
    "class Critical_new(nn.Module):\n",
    "    def __init__(self,  latent_vars, gen_dim, max_seq_len, annotated=False, res_layers=5):\n",
    "        super(Critical_new, self).__init__()\n",
    "        self.input_size = latent_vars.shape\n",
    "        self.output_size = gen_dim * max_seq_len\n",
    "        self.seq_len = max_seq_len\n",
    "        self.gen_dim = gen_dim\n",
    "        self.res_layer = res_layers\n",
    "        #self.conv1 = nn.Conv1d(3,6,5,1)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv1d(gen_dim,1,5,1, padding = 'same'),\n",
    "                        nn.ReLU())\n",
    "        self.con1 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con2 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con3 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con4 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.con5 = nn.Conv1d(gen_dim ,gen_dim,5,1, padding = 'same')\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv1d(1, gen_dim,5,1, padding = 'same'),\n",
    "                        nn.ReLU())\n",
    "        # self.con_array = [\n",
    "        #     self.con1,\n",
    "        #     self.con2,\n",
    "        #     self.con3,\n",
    "        #     self.con4,\n",
    "        #     self.con5,\n",
    "        # ]\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        _input = x\n",
    "        out = self.conv2(_input)\n",
    "        \n",
    "        out = self.con1(out)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con1(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con2(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con3(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con4(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        _input = out\n",
    "\n",
    "        out = self.con5(_input)\n",
    "        out = self.relu(out)\n",
    "        out = _input + 0.3 * out\n",
    "        # for i in range(self.res_layer):\n",
    "        #     output = self.res_block(_input.float(),i, 0.3)\n",
    "        #     _input = output\n",
    "        # #output = torch.reshape(output,(-1,1))\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        return out #self.sigmoid(output)\n",
    "    \n",
    "    def res_block(self,_input, index, downsample = 0):\n",
    "        res = _input\n",
    "        out = self.con_array[index](res)\n",
    "        out = self.relu(out)\n",
    "        if downsample:\n",
    "            res = res * downsample\n",
    "            out = out + res\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
